{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to estimate at least GPU count\n",
    "\n",
    "This script which typically simulates the GPU memory consumption flow is used to estimate the GPU minimum count by giving some parameters including rows and columns and others. The more precise parameters are, the more accurate the output is.\n",
    "\n",
    "| parameters |  |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| SINGLE_GPU_MEMORY_SIZE | The size of one gpu memory on device, you can get it by `nvidia-smi` |\n",
    "| NUM_OF_FEATURE_COLUMNS | The total feature columns of input dataset. |\n",
    "| NUM_OF_WEIGHT_COLUMNS | The total weight columns of input dataset. If no weight column, it should be set to 0. |\n",
    "| NUM_OF_GROUPS | the size of prediction per instance. This value is set to 1 for all tasks except multi-class classification. For multi-class classification, NUM_OF_GROUPS must be set to the number of classes |\n",
    "| SPARSITY | sparsity of input dataset. (1 - NON_ZEROR_COUNT(A)/TOTAL_COUNT_A) |\n",
    "| MAX_BIN | maximum number of discrete bins to bucket continuous features. Default is 16 |\n",
    "\n",
    "---\n",
    "\n",
    "- ROW_STRIDE\n",
    "\n",
    "As to ROW_STRIDE, which is the largest number of features/items across all rows the input dataset. \n",
    "You can calculate it by \n",
    "```shell\n",
    "cat xxxx | awk -F, '{$NF=\"\"; print $0}'  | sort -n -r | head -1 | awk '{for(i=0;i<NF;i++) if($i!=0) sum+=1} END {print sum}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input cell\n",
    "\n",
    "SINGLE_GPU_MEMORY_SIZE      = 32 * 1024 * 1024 * 1024   # one single GPU memory on device\n",
    "\n",
    "ROW_COUNT       = 72 * 1000 * 1000              # 72 million\n",
    "\n",
    "NUM_OF_FEATURE_COLUMNS = 3600        # the number of feature columns\n",
    "\n",
    "NUM_OF_WEIGHT_COLUMNS  = 0           # the number of weight columns, it should be 0 or 1\n",
    "\n",
    "NUM_OF_GROUPS   = 1           # number of prediction dimension\n",
    "assert NUM_OF_GROUPS > 0\n",
    "\n",
    "SPARSITY        = 0.5  #(1 - NON_ZEROR_COUNT(A)/TOTAL_COUNT_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below parameters can also affects the result, all of them are default values\n",
    "\n",
    "MAX_BIN         = 16     # max_bin default value. It is 256 in native xgboost, while it is 16 in xgboost-4j\n",
    "\n",
    "# ROW_STRIDE: it should be <= NUM_OF_FEATURE_COLUMNS\n",
    "# let's assume last column is feature column, so the ROW_STRIDE can be calculated with below script\n",
    "# cat xxxx | awk -F, '{$NF=\"\"; print $0}'  | sort -n -r | head -1 | awk '{for(i=0;i<NF;i++) if($i!=0) sum+=1} END {print sum}'\n",
    "ROW_STRIDE = NUM_OF_FEATURE_COLUMNS\n",
    "\n",
    "LABEL_COLUMNS   = 1      \n",
    "assert LABEL_COLUMNS == 1 # Currently, XGBoost only supports 1 feature column\n",
    "\n",
    "TOTAL_COLUMNS = NUM_OF_FEATURE_COLUMNS + NUM_OF_WEIGHT_COLUMNS + LABEL_COLUMNS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below cells are immutable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GPU count: 8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell is immutable\n",
    "import math\n",
    "\n",
    "# constant values\n",
    "SIZE_OF_uint64   = 8\n",
    "SIZE_OF_float    = 4\n",
    "SIZE_OF_Entry    = 8\n",
    "SIZE_OF_uint32_t = 4\n",
    "\n",
    "N_BINS     = MAX_BIN * NUM_OF_FEATURE_COLUMNS   # maximum value\n",
    "\n",
    "# Formula\n",
    "\n",
    "'''\n",
    "Please note that,\n",
    "Here both stages of cudf loading and dmatrix construction are skipped, that's because we've supported\n",
    "chunk loading, and at the same time the DMatrix memory is moved to CPU from GPU, which means we can tune chunk\n",
    "size to load any data size if CPU memory is enough.\n",
    "\n",
    "And GPU sketcher stage is also skipped, for that memory is temporary.\n",
    "'''\n",
    "\n",
    "rows_to_load = ROW_COUNT\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # Step 1 PredictRaw\n",
    "    mem_PredictRaw  = NUM_OF_GROUPS * rows_to_load * SIZE_OF_float\n",
    "\n",
    "    # Step 2 GetGradient\n",
    "    labels  = LABEL_COLUMNS * rows_to_load * SIZE_OF_float\n",
    "    weights = NUM_OF_WEIGHT_COLUMNS * rows_to_load * SIZE_OF_float\n",
    "    out_gpair = NUM_OF_GROUPS * rows_to_load * SIZE_OF_float * 2\n",
    "\n",
    "    mem_GetGradient =  labels + weights + out_gpair\n",
    "\n",
    "    # Step3 Ellpack\n",
    "    n_items = rows_to_load * NUM_OF_FEATURE_COLUMNS * (1-SPARSITY)\n",
    "    num_symbols = N_BINS + 1\n",
    "    num_row_symbols = n_items + 1\n",
    "\n",
    "    ellpack_gidx_row_buffer = 0  #inititalized value\n",
    "    compressed_size_bytes = math.log2(num_symbols) * ROW_STRIDE * rows_to_load  / 8\n",
    "\n",
    "    is_dense = SPARSITY == 0\n",
    "    if not is_dense: #it's csr dmatrix\n",
    "        item_compressed_size_bytes = math.log2(num_symbols) * n_items / 8\n",
    "        row_compressed_size_bytes = math.log2(num_row_symbols) * (rows_to_load + 1) / 8\n",
    "        if (item_compressed_size_bytes + row_compressed_size_bytes < compressed_size_bytes):\n",
    "            compressed_size_bytes = item_compressed_size_bytes\n",
    "            ellpack_gidx_row_buffer = row_compressed_size_bytes\n",
    "\n",
    "    ellpack_prediction_cache = rows_to_load * SIZE_OF_float\n",
    "    ellpack_gidx_buffer      = compressed_size_bytes\n",
    "    mem_Ellpack     =  ellpack_prediction_cache +  ellpack_gidx_row_buffer + ellpack_gidx_buffer\n",
    "\n",
    "    # Step4  RowPartitioner\n",
    "    ridx_a     = rows_to_load * SIZE_OF_uint32_t\n",
    "    ridx_b     = rows_to_load * SIZE_OF_uint32_t\n",
    "    position_a = rows_to_load * SIZE_OF_uint32_t\n",
    "    position_b = rows_to_load * SIZE_OF_uint32_t\n",
    "\n",
    "    mem_row_partitioner = ridx_a + ridx_b + position_a + position_b\n",
    "\n",
    "    # Step5 DeviceHistogram\n",
    "    mem_device_histogram = 1 * 1024 * 1024 * 1024  #1G\n",
    "\n",
    "    mem_total = mem_PredictRaw + mem_GetGradient + mem_Ellpack + mem_row_partitioner + mem_device_histogram\n",
    "    \n",
    "    gpu_count = math.ceil(mem_total * 1.0 / SINGLE_GPU_MEMORY_SIZE)\n",
    "    print(\"\\n GPU count: %d \\n\" % (gpu_count))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
