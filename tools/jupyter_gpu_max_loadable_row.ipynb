{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to calculate max loadable rows given GPU mem, columns\n",
    "\n",
    "This script which typically simulates the GPU memory consumption flow is used to estimate the max loadable data rows when some parameters including GPU memory and columns and others are given. The more precise parameters are, the more accurate the output is.\n",
    "\n",
    "| parameters |  |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| GPU_MEMORY | The total gpu memories on device, you can get it by `nvidia-smi` |\n",
    "| NUM_OF_FEATURE_COLUMNS | The total feature columns of input dataset. |\n",
    "| NUM_OF_WEIGHT_COLUMNS | The total weight columns of input dataset. If no weight column, it should be set to 0. |\n",
    "| NUM_OF_LABEL_COLUMNS | The total label columns of input dataset. Currently, XGBoost only supports 1 feature column. So no need to change this. |\n",
    "| NUM_OF_GROUPS | the size of prediction per instance. This value is set to 1 for all tasks except multi-class classification. For multi-class classification, NUM_OF_GROUPS must be set to the number of classes |\n",
    "| SPARSITY | sparsity of input dataset. (1 - NON_ZEROR_COUNT(A)/TOTAL_COUNT_A) |\n",
    "| MAX_BIN | maximum number of discrete bins to bucket continuous features. Default is 16 |\n",
    "\n",
    "---\n",
    "\n",
    "- ROW_STRIDE\n",
    "\n",
    "As to ROW_STRIDE, which is the largest number of features/items across all rows the input dataset. \n",
    "You can calculate it by \n",
    "```shell\n",
    "cat xxxx | awk -F, '{$NF=\"\"; print $0}'  | sort -n -r | head -1 | awk '{for(i=0;i<NF;i++) if($i!=0) sum+=1} END {print sum}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input cell\n",
    "\n",
    "GPU_MEMORY      = 12 * 1024 * 1024 * 1024 * 1   # total size of  GPU memory on device\n",
    "\n",
    "NUM_OF_FEATURE_COLUMNS = 17        # number of feature columns\n",
    "\n",
    "NUM_OF_WEIGHT_COLUMNS  = 0           # number of weight columns, it should be 0 or 1\n",
    "\n",
    "NUM_OF_GROUPS   = 1  # default is 1\n",
    "assert NUM_OF_GROUPS > 0\n",
    "\n",
    "SPARSITY        = 1  #(1 - NON_ZEROR_COUNT(A)/TOTAL_COUNT_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BIN         = 16     # max_bin default value, It is 256 in native xgboost, while it is 16 in xgboost-4j\n",
    "\n",
    "# ROW_STRIDE: it should be <= NUM_OF_FEATURE_COLUMNS\n",
    "# let's assume last column is feature column, so the ROW_STRIDE can be calculated with below script\n",
    "# cat xxxx | awk -F, '{$NF=\"\"; print $0}'  | sort -n -r | head -1 | awk '{for(i=0;i<NF;i++) if($i!=0) sum+=1} END {print sum}'\n",
    "ROW_STRIDE = 17  \n",
    "\n",
    "LABEL_COLUMNS   = 1\n",
    "assert LABEL_COLUMNS == 1 # Currently, XGBoost only supports 1 feature column\n",
    "\n",
    "TOTAL_COLUMNS = NUM_OF_FEATURE_COLUMNS + NUM_OF_WEIGHT_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below cells are immutable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values\n",
    "SIZE_OF_uint64   = 8\n",
    "SIZE_OF_float    = 4\n",
    "SIZE_OF_Entry    = 8\n",
    "SIZE_OF_uint32_t = 4\n",
    "\n",
    "N_BINS     = MAX_BIN * NUM_OF_FEATURE_COLUMNS   # maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max loadable rows:328087800 Given cols:18 on GPU:12 G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Formula\n",
    "\n",
    "'''\n",
    "Please note that,\n",
    "Here both stages of cudf loading and dmatrix construction are skipped, that's because we've supported\n",
    "chunk loading, and the DMatrix memory is moved to CPU from GPU, which means we can tune chunk\n",
    "size to load any data size if CPU memory is enough.\n",
    "\n",
    "And GPU sketcher stage is also skipped, for that memory is temporary.\n",
    "'''\n",
    "\n",
    "step = 100  # define step to 100 to speed up converge.\n",
    "\n",
    "rows_to_load = step\n",
    "while (True):\n",
    "    \n",
    "    # Step 1 PredictRaw\n",
    "    mem_PredictRaw  = NUM_OF_GROUPS * rows_to_load * SIZE_OF_float\n",
    "\n",
    "    # Step 2 GetGradient\n",
    "    labels  = LABEL_COLUMNS * rows_to_load * SIZE_OF_float\n",
    "    weights = NUM_OF_WEIGHT_COLUMNS * rows_to_load * SIZE_OF_float\n",
    "    out_gpair = NUM_OF_GROUPS * rows_to_load * SIZE_OF_float * 2\n",
    "\n",
    "    mem_GetGradient =  labels + weights + out_gpair\n",
    "\n",
    "    # Step3 Ellpack\n",
    "    n_items = rows_to_load * NUM_OF_FEATURE_COLUMNS * (1-SPARSITY)\n",
    "    num_symbols = N_BINS + 1\n",
    "    num_row_symbols = n_items + 1\n",
    "\n",
    "    ellpack_gidx_row_buffer = 0  #inititalized value\n",
    "    compressed_size_bytes = math.log2(num_symbols) * ROW_STRIDE * rows_to_load  / 8\n",
    "\n",
    "    is_dense = SPARSITY == 0\n",
    "    if not is_dense: #it's csr dmatrix\n",
    "        item_compressed_size_bytes = math.log2(num_symbols) * n_items / 8\n",
    "        row_compressed_size_bytes = math.log2(num_row_symbols) * (rows_to_load + 1) / 8\n",
    "        if (item_compressed_size_bytes + row_compressed_size_bytes < compressed_size_bytes):\n",
    "            compressed_size_bytes = item_compressed_size_bytes\n",
    "            ellpack_gidx_row_buffer = row_compressed_size_bytes\n",
    "\n",
    "    ellpack_prediction_cache = rows_to_load * SIZE_OF_float\n",
    "    ellpack_gidx_buffer      = compressed_size_bytes\n",
    "    mem_Ellpack     =  ellpack_prediction_cache +  ellpack_gidx_row_buffer + ellpack_gidx_buffer\n",
    "\n",
    "    # Step4  RowPartitioner\n",
    "    ridx_a     = rows_to_load * SIZE_OF_uint32_t\n",
    "    ridx_b     = rows_to_load * SIZE_OF_uint32_t\n",
    "    position_a = rows_to_load * SIZE_OF_uint32_t\n",
    "    position_b = rows_to_load * SIZE_OF_uint32_t\n",
    "\n",
    "    mem_row_partitioner = ridx_a + ridx_b + position_a + position_b\n",
    "\n",
    "    # Step5 DeviceHistogram\n",
    "    mem_device_histogram = 1 * 1024 * 1024 * 1024  #1G\n",
    "\n",
    "    mem_total = mem_PredictRaw + mem_GetGradient + mem_Ellpack + mem_row_partitioner + mem_device_histogram \n",
    "    if mem_total > GPU_MEMORY:\n",
    "        print(\"\\nMax loadable rows:%d Given cols:%d on GPU:%d G\\n\" % (rows_to_load, TOTAL_COLUMNS, GPU_MEMORY/1024/1024/1024))\n",
    "        break\n",
    "        loadable\n",
    "    rows_to_load += step  # speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
